---
title: "Gradiente"
author: "Alvaro Muñoz Jimenez"
date: "7/11/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

En primer lugar cargamos las librerías que vamos a utilizar.

```{r}
library(readr)
library(ISLR)
library(MASS)
library(dplyr)
library (here)
library(tidyverse)
```

Cargamos los datos correspondiendes a la base de datos "4_1_data".

```{r}
datos <- read_csv("4_1_data.csv")
```

# Visualización de datos

Observamos que el dataset a analizar contiene datos de tres variables recogidos en 100 observaciones.

```{r}
names(datos)
```

La variable "label" es dicotómica y señala si un alumno ha sido admitido (1) o si no ha sido admitido(0).

Las variables "score 1" y "score2" son las notas de la primera y de la segunda prueba.

Para evitar errores de sintaxis cambiamos los nombres a las variables del dataset.

```{r}
datos <- rename(datos,"score1" = `score-1`)
datos <- rename(datos,"score2" = `score-2`)
```

Observamos los principales estadísticos descriptivos de nuestra base de datos

```{r}
summary (datos)
```



#Ejercicio 1

Visualización gráfica de datos mediante el gráfico de dispersión:

```{r}
plot(datos$score1, datos$score2, col = as.factor(datos$label), xlab = "score1", ylab = "score2")
```

Observamos las notas correspondientes de las pruebas, en color negro los alumnos no admitidos y en color rojo los alumnos admitidos. 

Visualizamos los  histogramas de las dos pruebas y de los alumnos admitidos y no admitidos.

```{r}
hist(datos$score1)
hist(datos$score2)
hist(datos$label)
```

Establecemos una semilla

```{r}
set.seed(1000)
```

Contamos el número de filas y lo guardamos en el objeto "n":

```{r}
n <- nrow(datos)
```

Creamos un split aleatorio para train(70%) y para test(30%). 

```{r}
train_aleatorio <- sample(1:n, 0.70*n) 
datos.train <- datos[train_aleatorio,]
datos.test <- datos[-train_aleatorio,]
```

Train:

```{r}
x.train <- data.frame(rep(1,70), datos.train$score1, datos.train$score2) 
x <- as.matrix(x.train) #Esto sería x train
y <- as.matrix(datos.train$label) #Esto sería y train
```

Test:

```{r}
x.test <- data.frame(rep(1,30), datos.test$score1, datos.test$score2) 
x.test <- as.matrix(x.test) 
y.test <- as.matrix(datos.test$label)
```

Función Sigmoide:

```{r}
Sigmoid <- function(x) 
  1 / (1 + exp(-x))
```

Funcion de costes:

Su objetivo es tratar de optimizar los parámetros determinando el error entre el valor estimado y el valor real, es decir,encontrar los valores de los betas óptimos que minimicen el valor de la función de costes.Creamos la función:

```{r}
funcionCostes <- function(parametros, X, Y) {
  n <- nrow(X)
  g <- Sigmoid(X %*% parametros)
  J <- (1/n) * sum((-Y * log(g)) - ((1 - Y) * log(1 - g)))
  return(J)
}
  
```

Se calcula el coste inicial tomando como valor inicial de los parámetros cero. El objetivo es reducir ese coste.

```{r}
parametros <- rep(0, ncol(x))
```

Coste máximo:

```{r}
coste_inicial = funcionCostes(parametros, x, y)
coste_inicial

```

Creamos una función para obtener los parámetros óptimos:

```{r}
TestGradientDescent <- function(iterations = 1200, X, Y) {
  parametros <- rep(0, ncol(X)
                    )
  print(paste("Función inicial de costes: ", 
              convergence <- c(funcionCostes(parametros, X, Y)), sep = ""))
  parametros_optimizacion <- optim(par = parametros, fn = funcionCostes, X = X, Y = Y, 
                                   control = list(maxit = iterations))
 
  parametros <- parametros_optimizacion$par
  
  print(paste("Valor final de función de costes: ", 
              convergence <- c(funcionCostes(parametros, X, Y)), sep = ""))

 return(parametros) 
}
```

Ejecutamos la función para nuestros valores

```{r}
parametros_optimos <-TestGradientDescent(X = x, Y= y)
parametros_optimos

probabilidades <- Sigmoid((x.test %*% parametros_optimos)) 
probabilidades
```

Cut of

```{r}
probabilidades[probabilidades >= 0.5] <- 1
probabilidades[probabilidades< 0.5] <- 0
probabilidades

y.test
```

Creamos la matriz de confusión:

```{r}
table(y.test, probabilidades, dnn=c("Real", "Prediccion"))
```

Accuracy:

```{r}
accuracy<-100*sum(diag(table(y.test, probabilidades)))/sum(table(y.test, probabilidades)) 
accuracy
```

#Ejercicio 2

Sacamos el error del modelo para cada iteración 

```{r}
TestGradientDescent <- function(iterations = 1200, X, Y) {
  parametros <- rep(0, ncol(X))
  errores <- NULL
  for (iteracion in 1:iterations) {
    parametros_optimizacion <- optim(par = parametros, fn = funcionCostes, X = X, Y = Y, 
                                   control = list(maxit = iteracion))
    errores[iteracion]<- parametros_optimizacion$value
  }
 return(errores) 
}
```

Asignamos valores

```{r}
ejeY <- TestGradientDescent(400, X = x.test, Y = y.test)
ejex <- 1:400
```

Representamos gráficamente el error medio para cada iteración.

```{r}
plot(x = ejex, y=ejeY)
```

#Ejercicio 3

Combrobamos qué argumentos tiene la función "Optium"

```{r}
args(optim)
```

Observamos que existen diversos métodos para optimizar la función y su convergencia . Son los siguientes:


method = "Nelder-Mead", "BFGS", "CG", "L-BFGS-B", "SANN", "Brent"

En la salida de la función se explica si han convergido (valor 0) o no (valor 1).


Exploraremos ahora el método BFGS:

Hace uso tanto del gradiente como de una aproximación a la inversa de la matriz hessiana de la función para hacer una aproximación al cálculo de la segunda derivada. Como inconveniente resaltar que  es costoso computacionalmente para funciones de muchas variables. 

```{r}
parametros_optimizados1 <- optim(par = parametros, fn = funcionCostes, X = x, Y = y, method = "BFGS",lower=-Inf,upper=Inf,control = list(maxit = 60), hessian=TRUE) 

parametros_optimizados1
```
